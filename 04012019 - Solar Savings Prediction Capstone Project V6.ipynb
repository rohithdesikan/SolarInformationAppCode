{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Base Packages\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "# File IO and System Packages\n",
    "import dill\n",
    "\n",
    "# File Download from Website \n",
    "import requests\n",
    "\n",
    "# Pandas Formatting and Styling:\n",
    "pd.options.display.max_columns = 500\n",
    "pd.set_option('display.float_format',lambda x: '%.3f' % x)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Geo Packages\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "import geopy as gpy\n",
    "from geopy.geocoders import Nominatim\n",
    "import shapely\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "import shapefile\n",
    "import pysal as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data Visualization Packages\n",
    "\n",
    "# Matplotlib and Seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Polygon\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={'image.cmap': 'cubehelix'})\n",
    "sns.set_context('poster')\n",
    "figure(figsize=(20, 16))\n",
    "\n",
    "# Bokeh\n",
    "import bokeh as bk\n",
    "from bokeh.io import output_notebook, show, output_file, save\n",
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.models import (ColumnDataSource as cds, Plot, DatetimeAxis, PanTool, WheelZoomTool, HoverTool, \n",
    "                          tickers, BoxAnnotation, Panel, Range1d, LabelSet, Label, NumeralTickFormatter, \n",
    "                          LogColorMapper, GeoJSONDataSource, LinearColorMapper, ColorBar,\n",
    "                          LogTicker, BasicTicker, CategoricalColorMapper, FixedTicker, AdaptiveTicker)\n",
    "from bokeh.palettes import viridis, magma, inferno, cividis, Greens, Blues, PuRd, YlOrRd, YlOrBr, RdYlGn\n",
    "from bokeh.embed import file_html\n",
    "from bokeh.layouts import layout, gridplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Machine Learning Packages\n",
    "from sklearn import base\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################### 1.) INPUT ORIGINAL REPLICA DATASET #############################\n",
    "# Read in the full summary Replica File \n",
    "replica_file = 'seeds_ii_replica.csv'\n",
    "replica = pd.read_csv(replica_file,thousands=',',low_memory=False)\n",
    "\n",
    "# Get information about the dataset\n",
    "display(replica.shape)\n",
    "replica.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 2.) ADD IN A GPD FILE TO FIGURE OUT LATITUDE CENTROIDS\n",
    "latlong = gpd.read_file('/Users/rohithdesikan/Desktop/Data Analysis/General Data Sources/Low Income Solar/high_mf_own/high_mf_own.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latlong.iloc[:24253,:].to_csv('latlong1.csv')\n",
    "latlong.iloc[24253:48506,:].to_csv('latlong2.csv')\n",
    "latlong.iloc[48506:,:].to_csv('latlong3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 3.) INSERT $ SAVED PER YEAR DATA INTO DATASET #############################\n",
    "# Add in solar potential bill savings (This is y aka labels)\n",
    "lmi_savings = pd.read_csv('lmi_potential_electric_bill_savings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 4.) INSERT PV ROOFTOP POTENTIAL BY TRACT #############################\n",
    "pv_gen = pd.read_csv('pv_rooftop_tract_lmi_gwh.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################### 1.) CLEAN UP ORIGINAL REPLICA DATASET #############################\n",
    "\n",
    "# Drop unrequired columns \n",
    "drop_cols1 = ['gisjoin','state_fips','state_name','county_fips','tract_fips',\n",
    "              'aqi_max_description','company_na',\n",
    "              'aqi_90th_percentile_description','aqi_median_description',\n",
    "              'climate_zone','eia_id','avg_cbi_usd_p_w','avg_ibi_pct',\n",
    "              'hdd_std','hdd_ci','cdd_std','cdd_ci','lihtc_qualified']\n",
    "replica_drop = replica.drop(drop_cols1,axis='columns')\n",
    "\n",
    "# Drop columns that are more than 75% NAN\n",
    "replica_nans = replica_drop.dropna(axis='columns', thresh=int(0.75*len(replica)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.) Find annual consumption and annual savings instead of monthly and rename the columns (Multiply by 12*1.1 Safety)\n",
    "replica_nans['avg_monthly_consumption_kwh'] *= 12*1.1\n",
    "replica_nans['avg_monthly_bill_dlrs'] *= 12*1.1\n",
    "\n",
    "replica_nans.rename({'avg_monthly_consumption_kwh':'avg_yearly_consumption_kwh',\n",
    "           'avg_monthly_bill_dlrs':'avg_yearly_bill_dlrs'},\n",
    "            axis=1,inplace=True)\n",
    "\n",
    "# Move these specific columns to the end of the matrix\n",
    "cols1 = replica_nans.columns.tolist()\n",
    "n = int(cols1.index('avg_yearly_consumption_kwh'))\n",
    "cols1 = cols1[:n] + cols1[n+3:] + cols1[n:n+3]\n",
    "replica_nans = replica_nans[cols1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################### 3.) MATCH GEOID TO ZIP CODES WITH DATASET #2 #########################\n",
    "replica_geom = replica_nans.copy()\n",
    "mp = latlong['geometry']\n",
    "centroid_y = latlong['geometry'].centroid.y\n",
    "\n",
    "replica_geom['centroid_y'] = centroid_y\n",
    "replica_geom['polygons'] = mp\n",
    "\n",
    "cols1 = replica_geom.columns.tolist()\n",
    "n = int(cols1.index('centroid_x'))\n",
    "cols1 = cols1[:n+1]+ cols1[-2:] + cols1[n+1:-2]\n",
    "replica_geom = replica_geom[cols1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 3.) Sort and only choose the geoid and potential solar gen columns ##########\n",
    "pv_gen =  pv_gen[['geoid','lmi_mf_gwh', 'lmi_sf_gwh', 'lmi_rent_gwh', 'lmi_own_gwh']]\n",
    "replica_pv = replica_geom.merge(pv_gen,how='left',on='geoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################### 4.) Sort and only choose the geoid and potential annual bill savings columns ##########\n",
    "lmi_savings = lmi_savings[['geoid','lmi_potential_savings_dlrs_year']]\n",
    "replica_final = replica_pv.merge(lmi_savings,how='left',on='geoid')\n",
    "replica_final.rename({'lmi_potential_savings_dlrs_year':'Savings [$/yr]'},axis=1,inplace=True)\n",
    "replica_final['Savings [% Annual Income]'] = (replica_final['Savings [$/yr]']*100)/replica_final['hh_med_income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################## ########################## ########################## ########################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################# DILL REPLICA_FINAL TO PULL IN LATER\n",
    "# dill.dump(replica_final, open('replica_final.pkd', 'wb'))\n",
    "\n",
    "replica_final = dill.load(open('replica_final.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(replica_final.shape)\n",
    "display(replica_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create subsets of data for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chose the demographic subset of the data\n",
    "replica_cols = replica_final.columns.tolist()\n",
    "\n",
    "plys = replica_cols.index('polygons')\n",
    "cty = replica_cols.index('company_ty')\n",
    "locale = replica_cols.index('locale')\n",
    "\n",
    "r_base = replica_final.iloc[:,:plys]\n",
    "r_dems = replica_final.iloc[:,cty:locale+1]\n",
    "\n",
    "rdems = pd.concat([r_base,r_dems],axis=1)\n",
    "rdems[['Savings [$/yr]','Savings [% Annual Income]']] = replica_final.iloc[:,-2:]\n",
    "\n",
    "# Work with the Demographic Subset to get Mean HH Income and Total Population\n",
    "rdems2 = replica_final[['geoid','state_abbr','county_name','polygons','hh_med_income','pop_total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the lidar subset of the data\n",
    "r_lid = replica_final.iloc[:,:cty]\n",
    "r_end = replica_final.iloc[:,locale+1:]\n",
    "rlidar = pd.concat([r_lid,r_end],axis=1)\n",
    "\n",
    "# Work with the Lidar Subset to get dev roof area and MWH gen potential\n",
    "rlidar2 = rlidar.copy()\n",
    "\n",
    "\n",
    "# Choose the Multifamily vs Single Family subset of the data\n",
    "devp_m2_cols = [x for x in rlidar2.columns[rlidar2.columns.str.contains('devp_m2')]]\n",
    "hh_cols = [x for x in rlidar2.columns[rlidar2.columns.str.contains('hh')]]\n",
    "mwh_cols = [x for x in rlidar2.columns[rlidar2.columns.str.contains('mwh')]]\n",
    "\n",
    "rlidar2['hh'] = rlidar2[hh_cols].sum(axis=1)\n",
    "rlidar2['devp_m2'] = rlidar2[devp_m2_cols].sum(axis=1)\n",
    "rlidar2['mwh'] = rlidar2[mwh_cols].sum(axis=1)\n",
    "rlidar2 = rlidar2[['hh','devp_m2','mwh','Savings [% Annual Income]']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate:\n",
    "rplot = pd.concat([rdems2,rlidar2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization Inputs and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant style for all plots\n",
    "def style(p):\n",
    "        # Title \n",
    "        p.title.align = 'center'\n",
    "        p.title.text_font_size = '16pt'\n",
    "        p.title.text_font = 'sans serif'\n",
    "\n",
    "        # Axis titles\n",
    "        p.xaxis.axis_label_text_font_size = '12pt'\n",
    "        p.xaxis.axis_label_text_font_style = 'bold'\n",
    "        p.yaxis.axis_label_text_font_size = '12pt'\n",
    "        p.yaxis.axis_label_text_font_style = 'bold'\n",
    "\n",
    "        # Tick labels\n",
    "        p.xaxis.major_label_text_font_size = '10pt'\n",
    "        p.yaxis.major_label_text_font_size = '10pt'\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pick a state: (#####---- CODE ONLY WORKS FOR 1 STATE -----#####)\n",
    "statef = 'Texas' # Pick a state\n",
    "state_abbr = list(mapdf.loc[mapdf['STATE_NAME'].isin([statef]),'STATE_ABBR']) # Find the abbreviation\n",
    "\n",
    "# Find the map and replica as dataframes\n",
    "mapstate_df = mapdf[mapdf['STATE_NAME'].isin([statef])]\n",
    "rstate_df = rgeo[rgeo['state_abbr'] == state_abbr[0]]\n",
    "\n",
    "# Convert the state_dataframe to a json object\n",
    "mapjson = mapstate_df.to_json()\n",
    "rjson = rstate_df.to_json()\n",
    "\n",
    "# Find the latitude and longitude from the base map dataframe\n",
    "latmin = round(float(mapstate_df['geometry'].bounds['miny']))-1\n",
    "latmax = round(float(mapstate_df['geometry'].bounds['maxy']))+1\n",
    "longmin = round(float(mapstate_df['geometry'].bounds['minx']))-1\n",
    "longmax = round(float(mapstate_df['geometry'].bounds['maxx']))+1\n",
    "longdist = abs(longmin-longmax)\n",
    "latdist = latmax-latmin\n",
    "\n",
    "if longdist>latdist:\n",
    "    fig_width=1000\n",
    "    fig_height = int(fig_width*(latdist/longdist))\n",
    "elif latdist>longdist:\n",
    "    fig_height=1000\n",
    "    fig_width = int(fig_height*(longdist/latdist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replica_f = replica_final.fillna(method='ffill', axis = 0)\n",
    "\n",
    "# Find the columns within the data and send them to a list\n",
    "replica_cols = replica_f.columns.tolist()\n",
    "\n",
    "# Split at company_ty which is the split between lidar and demographic\n",
    "cty = replica_cols.index('company_ty')\n",
    "\n",
    "r_lidar = replica_f.iloc[:,:cty]\n",
    "r_lidar.drop(['state_abbr', 'county_name', 'centroid_x', 'centroid_y', 'polygons'], axis = 1, inplace = True)\n",
    "r_demographic = replica_f.iloc[:,cty:-2]\n",
    "\n",
    "savings = replica_f['Savings [$/yr]']\n",
    "# savings = [0 if a<0 else a for a in savings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Lidar data ML sets \n",
    "X_train_lidar, X_test_lidar, y_train_lidar, y_test_lidar = train_test_split(r_lidar, \n",
    "                                                                            savings, \n",
    "                                                                            test_size = 0.2, \n",
    "                                                                            random_state = 42)\n",
    "y_train_lidar_indices = y_train_lidar.index.values.tolist()\n",
    "y_test_lidar_indices = y_test_lidar.index.values.tolist()\n",
    "display(X_train_lidar.shape)\n",
    "display(len(y_train_lidar))\n",
    "display(X_test_lidar.shape)\n",
    "display(len(y_test_lidar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up demographic ML sets\n",
    "X_train_dem, X_test_dem,  y_train_dem, y_test_dem = train_test_split(r_demographic, \n",
    "                                                                    savings, \n",
    "                                                                    test_size = 0.2, \n",
    "                                                                    random_state = 42)\n",
    "\n",
    "\n",
    "train_one_hot = pd.get_dummies(X_train_dem[['company_ty',\n",
    "                                        'climate_zone_description', \n",
    "                                        'moisture_regime',\n",
    "                                        'locale']])\n",
    "X_train_dem = X_train_dem.join(train_one_hot)\n",
    "X_train_dem.drop(['company_ty', 'climate_zone_description', 'moisture_regime', 'locale'], axis = 1, inplace = True)\n",
    "\n",
    "test_one_hot = pd.get_dummies(X_test_dem[['company_ty',\n",
    "                                        'climate_zone_description', \n",
    "                                        'moisture_regime',\n",
    "                                        'locale']])\n",
    "X_test_dem = X_test_dem.join(test_one_hot)\n",
    "X_test_dem.drop(['company_ty', 'climate_zone_description', 'moisture_regime', 'locale'], axis = 1, inplace = True)\n",
    "\n",
    "y_train_dem_indices = y_train_lidar.index.values.tolist()\n",
    "y_test_dem_indices = y_test_lidar.index.values.tolist()\n",
    "\n",
    "display(X_train_dem.shape)\n",
    "display(len(y_train_dem))\n",
    "display(X_test_dem.shape)\n",
    "display(len(y_test_dem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_gbr = GradientBoostingRegressor(n_estimators= 600, \n",
    "                                      max_depth= 12, \n",
    "                                      min_samples_split= 2,\n",
    "                                      learning_rate= 0.02, \n",
    "                                      loss= 'lad')\n",
    "\n",
    "lidar_gbr.fit(X_train_lidar, y_train_lidar)\n",
    "ypred_train_gbr_lidar = lidar_gbr.predict(X_train_lidar)\n",
    "ypred_test_gbr_lidar = lidar_gbr.predict(X_test_lidar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Get Root Mean Squared Error (RMSE) value as the evaluation metric\n",
    "RMSE_train_gbr_lidar = metrics.mean_absolute_error(y_train_lidar, ypred_train_gbr_lidar)\n",
    "RMSE_test_gbr_lidar = metrics.mean_absolute_error(y_test_lidar, ypred_test_gbr_lidar)\n",
    "display((RMSE_train_gbr_lidar,RMSE_test_gbr_lidar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Plot train or test data to see how close it is\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pd.Series.rolling(pd.Series(y_train_lidar.values.tolist()), window=500).mean())\n",
    "plt.plot(pd.Series.rolling(pd.Series(ypred_train_gbr_lidar), window=500).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Plot train or test data to see how close it is\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pd.Series.rolling(pd.Series(y_test_lidar.values.tolist()), window=500).mean())\n",
    "plt.plot(pd.Series.rolling(pd.Series(ypred_test_gbr_lidar), window=500).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF Regression for Demographic Data\n",
    "demographic_rf = RandomForestRegressor(n_estimators = 300, max_depth = 8)\n",
    "demographic_rf.fit(X_train_dem, y_train_dem)\n",
    "ypred_train_dem = demographic_rf.predict(X_train_dem)\n",
    "ypred_test_dem = demographic_rf.predict(X_test_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Get Root Mean Squared Error (RMSE) value as the evaluation metric\n",
    "RMSE_train_dem = metrics.mean_absolute_error(y_train_dem, ypred_train_dem)\n",
    "RMSE_test_dem = metrics.mean_absolute_error(y_test_dem, ypred_test_dem)\n",
    "display((RMSE_train_dem,RMSE_test_dem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pd.Series.rolling(pd.Series(y_train_dem), window=500).mean())\n",
    "plt.plot(pd.Series.rolling(pd.Series(ypred_train_dem), window=500).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pd.Series.rolling(pd.Series(y_test_dem), window=500).mean())\n",
    "plt.plot(pd.Series.rolling(pd.Series(ypred_test_dem), window=500).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dill.dump(ypred_train_gbr_lidar, open('ypred_train_gbr_lidar.pkd', 'wb'))\n",
    "# dill.dump(ypred_test_gbr_lidar, open('ypred_test_gbr_lidar.pkd', 'wb'))\n",
    "# dill.dump(ypred_train_dem, open('ypred_train_dem.pkd', 'wb'))\n",
    "# dill.dump(ypred_test_dem, open('ypred_test_dem.pkd', 'wb'))\n",
    "\n",
    "ypred_train_gbr_lidar = dill.load(open('ypred_train_gbr_lidar.pkd', 'rb'))\n",
    "ypred_test_gbr_lidar = dill.load(open('ypred_test_gbr_lidar.pkd', 'rb'))\n",
    "ypred_train_dem = dill.load(open('ypred_train_dem.pkd', 'rb'))\n",
    "ypred_test_dem = dill.load(open('ypred_test_dem.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ensemble = np.vstack((ypred_train_gbr_lidar, ypred_train_dem)).T\n",
    "y_train_ensemble = y_train_lidar.values.tolist()\n",
    "X_test_ensemble = np.vstack((ypred_test_gbr_lidar, ypred_test_dem)).T\n",
    "y_test_ensemble = y_test_lidar.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the ensemble model\n",
    "ensemble_linear = Ridge(alpha = 2, fit_intercept = True)\n",
    "ensemble_linear.fit(X_train_ensemble, y_train_ensemble)\n",
    "ypred_train_ensemble = ensemble_linear.predict(X_train_ensemble)\n",
    "ypred_test_ensemble = ensemble_linear.predict(X_test_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Get Root Mean Squared Error (RMSE) value as the evaluation metric\n",
    "RMSE_train_ensemble = metrics.mean_absolute_error(y_train_ensemble, ypred_train_ensemble)\n",
    "RMSE_test_ensemble = metrics.mean_absolute_error(y_test_ensemble, ypred_test_ensemble)\n",
    "display((RMSE_train_ensemble, RMSE_test_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pd.Series.rolling(pd.Series(y_train_ensemble), window=500).mean())\n",
    "plt.plot(pd.Series.rolling(pd.Series(ypred_train_ensemble), window=500).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pd.Series.rolling(pd.Series(y_test_ensemble), window=500).mean())\n",
    "plt.plot(pd.Series.rolling(pd.Series(ypred_test_ensemble), window=500).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_irradiation_cols = [x for x in replica_final.columns[replica_final.columns.str.contains('gwh')]]\n",
    "replica_final['gwh'] = replica_final[solar_irradiation_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replica_small = replica_final[['geoid', 'state_abbr', 'county_name' , 'polygons', 'avg_yearly_bill_dlrs', 'gwh', 'Savings [$/yr]']]\n",
    "replica_train = replica_small.iloc[y_train_lidar_indices, :]\n",
    "replica_train['Savings_Predictions'] = ypred_train_ensemble\n",
    "\n",
    "replica_test = replica_small.iloc[y_test_lidar_indices, :]\n",
    "replica_test['Savings_Predictions'] = ypred_test_ensemble\n",
    "replica_plot = pd.concat([replica_train, replica_test], axis = 0)\n",
    "\n",
    "# Turning replica data frame into a geopandas dataframe\n",
    "replica_plot.rename({'polygons':'geometry'},inplace=True,axis=1)\n",
    "replica_geoplot = gpd.GeoDataFrame(replica_plot, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot entire USA Map on Matplotlib and each state on Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA Basemap File\n",
    "file = '/Users/rohithdesikan/Desktop/Data Analysis/The Data Incubator/Capstone Project/states_21basic/states.shp'\n",
    "map_df = gpd.read_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Matplotlib Single Variable Plot of the entire US\n",
    "basem = map_df.plot(figsize=(300, 200), color='white', edgecolor='black')\n",
    "\n",
    "replica_geoplot.plot(ax=basem,column='Savings_Predictions',cmap='YlGn',scheme='quantiles',legend=True)\n",
    "\n",
    "\n",
    "# ALL OF USA\n",
    "plt.xlim(-125,-65)\n",
    "plt.ylim(25,50)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('USA Solar Savings Basemap.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the geoJSON datasource\n",
    "rjson = replica_few_plot.to_json()\n",
    "\n",
    "geo_source = GeoJSONDataSource(geojson=rjson)\n",
    "\n",
    "# Bokeh Plot of Predictions\n",
    "n=6 # Number of colors on the choropleth and legend\n",
    "\n",
    "# Set the colormapper\n",
    "Greens1 = Greens[n]\n",
    "Greens2 = Greens1[::-1]\n",
    "cmapper = LinearColorMapper(palette=Greens2,\n",
    "                            low = replica_geoplot['Savings_Predictions'].min(),\n",
    "                            high = replica_geoplot['Savings_Predictions'].max())\n",
    "\n",
    "\n",
    "low = replica_geoplot['Savings_Predictions'].min()\n",
    "high = replica_geoplot['Savings_Predictions'].max()\n",
    "diff = high - low\n",
    "ticks = [diff/6, diff/3, diff/2, 2*diff/3, 5*diff/6, diff]\n",
    "    \n",
    "color_bar = bk.models.ColorBar(color_mapper=cmapper, ticker=FixedTicker(minor_ticks = [], ticks = ticks),\n",
    "                 label_standoff=12, border_line_color=None, location=(0,0))\n",
    "\n",
    "# Set up the figure \n",
    "p = figure(\n",
    "    title='Savings Predictions', \n",
    "    x_axis_location=None, \n",
    "    y_axis_location=None,\n",
    "    x_range=(-125,-70),\n",
    "    y_range=(31,43),\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\"\n",
    ")\n",
    "\n",
    "# Disable grid lines\n",
    "p.grid.grid_line_color = None\n",
    "\n",
    "# Plot the required variable\n",
    "p.patches('xs', \n",
    "          'ys', \n",
    "          source=geo_source,\n",
    "          fill_color={'field': 'Savings_Predictions', 'transform': cmapper},\n",
    "          fill_alpha=0.4, \n",
    "          line_color=\"white\", \n",
    "          line_width=1)\n",
    "\n",
    "p.add_tools(HoverTool(\n",
    "    tooltips= [\n",
    "    (\"State\",'@state_abbr'),\n",
    "    (\"County Name\", \"@county_name\"),\n",
    "    (\"Savings Predictions\", \"@Savings_Predictions{int}\"),\n",
    "    (\"(Long, Lat)\", \"($x, $y)\")\n",
    "    ],\n",
    "    formatters={\n",
    "    \"@State\":\"printf\",\n",
    "    \"@county_name\": \"printf\",\n",
    "    \"@Savings_Predictions\": \"numeral\",\n",
    "    \"($x, $y)\": \"numeral\"\n",
    "    },\n",
    "    mode='mouse'\n",
    "))\n",
    "\n",
    "p.add_layout(color_bar, 'right')\n",
    "# p.legend.location = \"top_right\"\n",
    "# p.legend.click_policy=\"hide\"\n",
    "\n",
    "style(p)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
